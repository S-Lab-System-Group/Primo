{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31134213",
   "metadata": {},
   "source": [
    "## Primo + Clara-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d3e20",
   "metadata": {},
   "source": [
    "### Clara-AI Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "source_train, target_train, source_ptest, target_ptest, source_ntest, target_ntest, click_dict = pd.read_pickle(\n",
    "    \"embedding.pickle\"\n",
    ")\n",
    "\n",
    "# Create a svm Classifier\n",
    "neigh = svm.SVC(kernel=\"linear\")  # Linear Kernel kernel='linear'\n",
    "neigh.fit(source_train, target_train)\n",
    "\n",
    "npred, ppred = neigh.predict(source_ntest), neigh.predict(source_ptest)\n",
    "summation_n, summation_p = npred.sum(), ppred.sum()\n",
    "total_n, total_p = len(source_ntest), len(source_ptest)\n",
    "fp_index, fn_index = np.argwhere(npred == 1), np.argwhere(ppred == 0)\n",
    "pred, y = np.append(npred, ppred), np.append(target_ntest, target_ptest)\n",
    "X = np.concatenate((source_ntest, source_ptest), axis=0)\n",
    "\n",
    "fn_source, fp_source = (\n",
    "    pd.DataFrame([source_ptest[i[0]] for i in fn_index]),\n",
    "    pd.DataFrame([source_ntest[i[0]] for i in fp_index]),\n",
    ")\n",
    "fn_target, fp_target = [target_ptest[i[0]] for i in fn_index], [target_ntest[i[0]] for i in fp_index]\n",
    "X_train = pd.DataFrame(source_train)\n",
    "y_train = pd.DataFrame(target_train)\n",
    "\n",
    "print(f\"FalsePositive index: {fp_index}\")\n",
    "print(f\"FalseNegative index: {fn_index}\")\n",
    "print(\"\")\n",
    "\n",
    "for key in click_dict:\n",
    "    item = click_dict[key][2]\n",
    "    if neigh.predict([item]) == [1]:\n",
    "        print(key[: key.find(\":\")], \"\\t crc hash accelerating opportunity found!!!!\")\n",
    "print(\"\")\n",
    "print(f\"Precision: \\t{precision_score(y, pred):.3f}\")\n",
    "print(f\"Recall: \\t{recall_score(y, pred):.3f}\")\n",
    "print(f\"F1  Score: \\t{f1_score(y, pred):.3f}\")\n",
    "# plot_roc_curve(neigh, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976ded4",
   "metadata": {},
   "source": [
    "### Primo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c47285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from primo.model import PrimoClassifier\n",
    "\n",
    "\"\"\"For fast result reprodcution, we disable HPO and model selection. Use specific model type and configuration.\"\"\"\n",
    "\n",
    "pram = PrimoClassifier(model=\"PrAM\", hpo=None)\n",
    "pram.fit(source_train, target_train)\n",
    "\n",
    "npred, ppred = pram.predict(source_ntest), pram.predict(source_ptest)\n",
    "summation_n, summation_p = npred.sum(), ppred.sum()\n",
    "total_n, total_p = len(source_ntest), len(source_ptest)\n",
    "fp_index, fn_index = np.argwhere(npred == 1), np.argwhere(ppred == 0)\n",
    "pred, y = np.append(npred, ppred), np.append(target_ntest, target_ptest)\n",
    "X = np.concatenate((source_ntest, source_ptest), axis=0)\n",
    "\n",
    "print(f\"FalsePositive index: {fp_index}\")\n",
    "print(f\"FalseNegative index: {fn_index}\")\n",
    "print(\"\")\n",
    "\n",
    "for key in click_dict:\n",
    "    item = click_dict[key][2]\n",
    "    if pram.predict([item]) == [1]:\n",
    "        print(key[: key.find(\":\")], \"\\t crc hash accelerating opportunity found!!!!\")\n",
    "print(\"\")\n",
    "print(f\"Precision: \\t{precision_score(y, pred):.3f}\")\n",
    "print(f\"Recall: \\t{recall_score(y, pred):.3f}\")\n",
    "print(f\"F1  Score: \\t{f1_score(y, pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98088d17",
   "metadata": {},
   "source": [
    "### Counterfactual Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5be4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FN analysis\n",
    "fn_df = fn_source.loc[:, (fn_source == 1).any(axis=0)]\n",
    "fn_df.index = fn_index.reshape(1,-1)[0]\n",
    "fn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a59894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP analysis\n",
    "fp_df = fp_source.loc[:, (fp_source == 1).any(axis=0)]\n",
    "fp_df.index = fp_index.reshape(1,-1)[0]\n",
    "fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2da439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from primo.post_optim import find_counterfactual\n",
    "\n",
    "find_counterfactual(pram, fn_source.iloc[3].values.reshape(1,-1), y_target=0, X_refer=X_train, y_refer=y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833acef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_counterfactual(pram, fp_source.iloc[0].values.reshape(1,-1), y_target=0, X_refer=X_train, y_refer=y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pram.prModel.gam.additive_terms_[84][2] = 6  # Correct FN [30]\n",
    "pram.prModel.gam.additive_terms_[0][2] = -6  # Correct FP [584]\n",
    "\n",
    "\"\"\"Evaluation\"\"\"\n",
    "npred, ppred = pram.predict(source_ntest), pram.predict(source_ptest)\n",
    "summation_n, summation_p = npred.sum(), ppred.sum()\n",
    "total_n, total_p = len(source_ntest), len(source_ptest)\n",
    "fp_index, fn_index = np.argwhere(npred == 1), np.argwhere(ppred == 0)\n",
    "pred, y = np.append(npred, ppred), np.append(target_ntest, target_ptest)\n",
    "X = np.concatenate((source_ntest, source_ptest), axis=0)\n",
    "\n",
    "print(f\"FalsePositive index: {fp_index}\")\n",
    "print(f\"FalseNegative index: {fn_index}\")\n",
    "print(\"\")\n",
    "\n",
    "for key in click_dict:\n",
    "    item = click_dict[key][2]\n",
    "    if pram.predict([item]) == [1]:\n",
    "        print(key[: key.find(\":\")], \"\\t crc hash accelerating opportunity found!!!!\")\n",
    "print(\"\")\n",
    "print(f\"Precision: \\t{precision_score(y, pred):.3f}\")\n",
    "print(f\"Recall: \\t{recall_score(y, pred):.3f}\")\n",
    "print(f\"F1  Score: \\t{f1_score(y, pred):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "220ec35765b29755edcb2bebb81fcd0a7cd14de4fe69e028c30e7300de3e2819"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
